```python
import keras
from keras.layers import Input, LSTM, Dense, Embedding
from keras.models import Model

tweet_a = Input(shape=(None,))
tweet_b = Input(shape=(None,))
e1 = Embedding(10000, 128)(tweet_a)
e2 = Embedding(10000, 128)(tweet_b)

# This layer can take as input a matrix
# and will return a vector of size 64
shared_lstm = LSTM(64)

# When we reuse the same layer instance
# multiple times, the weights of the layer
# are also being reused
# (it is effectively *the same* layer)
encoded_a = shared_lstm(e1)
encoded_b = shared_lstm(e2)

# We can then concatenate the two vectors:
merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)

# And add a logistic regression on top
predictions = Dense(1, activation='sigmoid')(merged_vector)

# We define a trainable model linking the
# tweet inputs to the predictions
model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

model.fit([q1, q2], df.label.values.reshape(-1, 1), batch_size=512, epochs=5, validation_split=0.25)

"""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, None, 128)    1280000     input_23[0][0]                   
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, None, 128)    1280000     input_24[0][0]                   
__________________________________________________________________________________________________
lstm_13 (LSTM)                  (None, 64)           49408       embedding_5[0][0]                
                                                                 embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 128)          0           lstm_13[0][0]                    
                                                                 lstm_13[1][0]                    
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            129         concatenate_9[0][0]              
==================================================================================================
Total params: 2,609,537
Trainable params: 2,609,537
Non-trainable params: 0
__________________________________________________________________________________________________
"""
```
